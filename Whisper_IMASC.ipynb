{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "view-in-github",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Vaibhavs10/fast-whisper-finetuning/blob/main/Whisper_w_PEFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cefac89",
   "metadata": {
    "id": "5cefac89"
   },
   "source": [
    "# Fine-tune Whisper (large) with LoRA & BNB powerd by PEFT ‚ö°Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2MvOaXjUjK71",
   "metadata": {
    "id": "2MvOaXjUjK71"
   },
   "source": [
    "With the environment now set up, let's try to secure a decent GPU for our Colab! Unfortunately, it's becoming much harder to get access to a good GPU with the free version of Google Colab. However, with Google Colab Pro one should have no issues in being allocated a V100 or P100 GPU.\n",
    "\n",
    "To get a GPU, click _Runtime_ -> _Change runtime type_, then change _Hardware accelerator_ from _None_ to _GPU_.\n",
    "\n",
    "We can verify that we've been assigned a GPU and view its specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5636f14b-182e-48b7-bd9f-d11a5cf80661",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-_1dgx205\n",
      "  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-_1dgx205\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 29a2b1420633d322140062d7c76b807f41fb90aa\n",
      "\u001b[33m  WARNING: Value for prefixed-purelib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /tmp/pip-build-env-vscz3o7y/normal/lib/python3.10/site-packages\n",
      "  sysconfig: /tmp/pip-build-env-vscz3o7y/normal/local/lib/python3.10/dist-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /tmp/pip-build-env-vscz3o7y/normal/lib/python3.10/site-packages\n",
      "  sysconfig: /tmp/pip-build-env-vscz3o7y/normal/local/lib/python3.10/dist-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = False\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = '/tmp/pip-build-env-vscz3o7y/normal'\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-purelib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /tmp/pip-build-env-vscz3o7y/overlay/lib/python3.10/site-packages\n",
      "  sysconfig: /tmp/pip-build-env-vscz3o7y/overlay/local/lib/python3.10/dist-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /tmp/pip-build-env-vscz3o7y/overlay/lib/python3.10/site-packages\n",
      "  sysconfig: /tmp/pip-build-env-vscz3o7y/overlay/local/lib/python3.10/dist-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = False\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = '/tmp/pip-build-env-vscz3o7y/overlay'\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting evaluate\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 84 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jiwer\n",
      "  Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
      "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.16.2-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.2 MB 29.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (3.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (6.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (0.14.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 330 kB 109.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (1.22.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (0.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (2023.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (23.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (4.65.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.5.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.14.5)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.3)\n",
      "Collecting rapidfuzz<4,>=3\n",
      "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.4 MB 100.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.10.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.7.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.0)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4.2)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.10/dist-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: numba>=0.45.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.56.4+1.g5f1bc7084)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.15.1)\n",
      "Collecting appdirs>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (65.5.1)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.39.2-py2.py3-none-any.whl (254 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 254 kB 84.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.37)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (10.0.1.dev0+ga6eabc2b.d20230609)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (3.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0.dev0) (4.6.3)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.45.1->librosa) (0.39.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (3.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.0.dev0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.0.dev0) (2023.5.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->librosa) (3.1.0)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.8 MB 101.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.37.0.dev0-py3-none-any.whl size=8337212 sha256=ed16277964fd254c0533936c9aa061c5b05367301490b0724aff7dd0b3a3142e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-km1v8_j1/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
      "Successfully built transformers\n",
      "Installing collected packages: huggingface-hub, tokenizers, setproctitle, sentry-sdk, responses, rapidfuzz, docker-pycreds, appdirs, wandb, transformers, jiwer, evaluate\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "\u001b[33m    WARNING: Value for bin_prefix does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "    distutils: /usr/bin\n",
      "    sysconfig: /usr/local/bin\u001b[0m\n",
      "\u001b[33m    WARNING: Additional context:\n",
      "    user = False\n",
      "    home = None\n",
      "    root = None\n",
      "    prefix = None\u001b[0m\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.14.1\n",
      "    Uninstalling tokenizers-0.14.1:\n",
      "      Successfully uninstalled tokenizers-0.14.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.34.0\n",
      "    Uninstalling transformers-4.34.0:\n",
      "      Successfully uninstalled transformers-4.34.0\n",
      "Successfully installed appdirs-1.4.4 docker-pycreds-0.4.0 evaluate-0.4.1 huggingface-hub-0.20.2 jiwer-3.0.3 rapidfuzz-3.6.1 responses-0.18.0 sentry-sdk-1.39.2 setproctitle-1.3.3 tokenizers-0.15.0 transformers-4.37.0.dev0 wandb-0.16.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!  pip install git+https://github.com/huggingface/transformers.git evaluate jiwer librosa soundfile wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2kBtM9XSjKE5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2kBtM9XSjKE5",
    "outputId": "5bc5af3d-ccf8-480b-9448-415d12106ad5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 13 09:58:28 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  Off  | 00000000:01:01.0 Off |                    0 |\n",
      "| N/A   28C    P0    32W / 250W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6WwnavbBuezQ",
   "metadata": {
    "id": "6WwnavbBuezQ"
   },
   "source": [
    "Alrighty! Let's configure our environment to ensure it uses the GPU provided by Colab to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1da5fff",
   "metadata": {
    "id": "e1da5fff",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4qdVEC0yHFqI",
   "metadata": {
    "id": "4qdVEC0yHFqI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhello34\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "OliYeGoDHQ_t",
   "metadata": {
    "id": "OliYeGoDHQ_t",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"wandb_whisper_e2e\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a528c1a",
   "metadata": {
    "id": "8a528c1a"
   },
   "source": [
    "We strongly advise you to upload model checkpoints directly the [Hugging Face Hub](https://huggingface.co/)\n",
    "whilst training. The Hub provides:\n",
    "- Integrated version control: you can be sure that no model checkpoint is lost during training.\n",
    "- Tensorboard logs: track important metrics over the course of training.\n",
    "- Model cards: document what a model does and its intended use cases.\n",
    "- Community: an easy way to share and collaborate with the community!\n",
    "\n",
    "Linking the notebook to the Hub is straightforward - it simply requires entering your Hub authentication token when prompted. Find your Hub authentication token [here](https://huggingface.co/settings/tokens):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed0OpduhX2JF",
   "metadata": {
    "id": "ed0OpduhX2JF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gG-2lYDPw3uW",
   "metadata": {
    "id": "gG-2lYDPw3uW"
   },
   "source": [
    "Next up, we define Whisper model checkpoints and task details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mJ9M1WKhu0KM",
   "metadata": {
    "id": "mJ9M1WKhu0KM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"openai/whisper-medium\"\n",
    "task = \"transcribe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EuhppXc9xAt2",
   "metadata": {
    "id": "EuhppXc9xAt2"
   },
   "source": [
    "Lastly, we define the dataset details, including the language we'd like to fine-tune Whisper on too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7sE0FPf7w-he",
   "metadata": {
    "id": "7sE0FPf7w-he",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = \"thennal/IMaSC\"\n",
    "language = \"Malayalam\"\n",
    "language_abbr = \"ml\" # Short hand code for the language we want to fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2787582-554f-44ce-9f38-4180a5ed6b44",
   "metadata": {
    "id": "a2787582-554f-44ce-9f38-4180a5ed6b44",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'speaker', 'audio'],\n",
      "        num_rows: 27578\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'speaker', 'audio'],\n",
      "        num_rows: 6895\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(dataset_name, split=\"train[0%:80%]\", use_auth_token=True)\n",
    "common_voice[\"test\"] = load_dataset(dataset_name,  split=\"train[80%:100%]\", use_auth_token=True)\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b1c56",
   "metadata": {
    "id": "805b1c56"
   },
   "source": [
    "Most ASR datasets only provide input audio samples (`audio`) and the\n",
    "corresponding transcribed text (`sentence`). Common Voice contains additional\n",
    "metadata information, such as `accent` and `locale`, which we can disregard for ASR.\n",
    "Keeping the notebook as general as possible, we only consider the input audio and\n",
    "transcribed text for fine-tuning, discarding the additional metadata information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20ba635d-518c-47ac-97ee-3cad25f1e0ce",
   "metadata": {
    "id": "20ba635d-518c-47ac-97ee-3cad25f1e0ce",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# common_voice = common_voice.remove_columns(\n",
    "#     [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"]\n",
    "# )\n",
    "\n",
    "# print(common_voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d63b2d2-f68a-4d74-b7f1-5127f6d16605",
   "metadata": {
    "id": "2d63b2d2-f68a-4d74-b7f1-5127f6d16605"
   },
   "source": [
    "## Prepare Feature Extractor, Tokenizer and Data\n",
    "\n",
    "The ASR pipeline can be de-composed into three stages:\n",
    "1. A feature extractor which pre-processes the raw audio-inputs\n",
    "2. The model which performs the sequence-to-sequence mapping\n",
    "3. A tokenizer which post-processes the model outputs to text format\n",
    "\n",
    "In ü§ó Transformers, the Whisper model has an associated feature extractor and tokenizer,\n",
    "called [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor)\n",
    "and [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer)\n",
    "respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5",
   "metadata": {
    "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6",
   "metadata": {
    "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gIaGxWbXkcrC",
   "metadata": {
    "id": "gIaGxWbXkcrC"
   },
   "source": [
    "To simplify using the feature extractor and tokenizer, we can _wrap_ both into a single `WhisperProcessor` class. This processor object can be used on the audio inputs and model predictions as required.\n",
    "In doing so, we only need to keep track of two objects during training:\n",
    "the `processor` and the `model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6",
   "metadata": {
    "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381acd09-0b0f-4d04-9eb3-f028ac0e5f2c",
   "metadata": {
    "id": "381acd09-0b0f-4d04-9eb3-f028ac0e5f2c"
   },
   "source": [
    "### Prepare Data\n",
    "\n",
    "Let's print the first example of the Common Voice dataset to see\n",
    "what form the data is in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e6b0ec5-0c94-4e2c-ae24-c791be1b2255",
   "metadata": {
    "id": "6e6b0ec5-0c94-4e2c-ae24-c791be1b2255",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '‡¥∏‡µº‡¥µ‡µç‡¥µ‡¥ï‡¥≤‡¥æ‡¥∂‡¥æ‡¥≤ ‡¥µ‡µà‡¥∏‡µç ‡¥ö‡¥æ‡µª‡¥∏‡¥≤‡µº ‡¥°‡µã. ‡¥ö‡¥®‡µç‡¥¶‡µç‡¥∞‡¥¨‡¥æ‡¥¨‡µÅ‡¥µ‡¥ø‡¥®‡µÅ‡¥Ç ‡¥∏‡¥Ç‡¥≠‡¥µ‡¥Ç ‡¥§‡¥≤‡¥µ‡µá‡¥¶‡¥®‡¥Ø‡¥æ‡¥µ‡µÅ‡¥ï‡¥Ø‡¥æ‡¥£‡µç', 'speaker': 'Sonia', 'audio': {'path': None, 'array': array([ 0.00921631,  0.00930786,  0.00939941, ..., -0.00497437,\n",
      "       -0.00497437, -0.00497437]), 'sampling_rate': 16000}}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a679f05-063d-41b3-9b58-4fc9c6ccf4fd",
   "metadata": {
    "id": "5a679f05-063d-41b3-9b58-4fc9c6ccf4fd"
   },
   "source": [
    "Since\n",
    "our input audio is sampled at 48kHz, we need to _downsample_ it to\n",
    "16kHz prior to passing it to the Whisper feature extractor, 16kHz being the sampling rate expected by the Whisper model.\n",
    "\n",
    "We'll set the audio inputs to the correct sampling rate using dataset's\n",
    "[`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column)\n",
    "method. This operation does not change the audio in-place,\n",
    "but rather signals to `datasets` to resample audio samples _on the fly_ the\n",
    "first time that they are loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6adf468e-8b3d-44b3-8340-0599bbe6ef01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 ‡¥∏‡µº‡¥µ‡µç‡¥µ‡¥ï‡¥≤‡¥æ‡¥∂‡¥æ‡¥≤ ‡¥µ‡µà‡¥∏‡µç ‡¥ö‡¥æ‡µª‡¥∏‡¥≤‡µº ‡¥°‡µã. ‡¥ö‡¥®‡µç‡¥¶‡µç‡¥∞‡¥¨‡¥æ‡¥¨‡µÅ‡¥µ‡¥ø‡¥®‡µÅ‡¥Ç ‡¥∏‡¥Ç‡¥≠‡¥µ‡¥Ç ‡¥§‡¥≤‡¥µ‡µá‡¥¶‡¥®‡¥Ø‡¥æ‡¥µ‡µÅ‡¥ï‡¥Ø‡¥æ‡¥£‡µç\n",
      "Decoded w/ special:    <|startoftranscript|><|ml|><|transcribe|><|notimestamps|>‡¥∏‡µº‡¥µ‡µç‡¥µ‡¥ï‡¥≤‡¥æ‡¥∂‡¥æ‡¥≤ ‡¥µ‡µà‡¥∏‡µç ‡¥ö‡¥æ‡µª‡¥∏‡¥≤‡µº ‡¥°‡µã. ‡¥ö‡¥®‡µç‡¥¶‡µç‡¥∞‡¥¨‡¥æ‡¥¨‡µÅ‡¥µ‡¥ø‡¥®‡µÅ‡¥Ç ‡¥∏‡¥Ç‡¥≠‡¥µ‡¥Ç ‡¥§‡¥≤‡¥µ‡µá‡¥¶‡¥®‡¥Ø‡¥æ‡¥µ‡µÅ‡¥ï‡¥Ø‡¥æ‡¥£‡µç<|endoftext|>\n",
      "Decoded w/out special: ‡¥∏‡µº‡¥µ‡µç‡¥µ‡¥ï‡¥≤‡¥æ‡¥∂‡¥æ‡¥≤ ‡¥µ‡µà‡¥∏‡µç ‡¥ö‡¥æ‡µª‡¥∏‡¥≤‡µº ‡¥°‡µã. ‡¥ö‡¥®‡µç‡¥¶‡µç‡¥∞‡¥¨‡¥æ‡¥¨‡µÅ‡¥µ‡¥ø‡¥®‡µÅ‡¥Ç ‡¥∏‡¥Ç‡¥≠‡¥µ‡¥Ç ‡¥§‡¥≤‡¥µ‡µá‡¥¶‡¥®‡¥Ø‡¥æ‡¥µ‡µÅ‡¥ï‡¥Ø‡¥æ‡¥£‡µç\n",
      "Are equal:             True\n"
     ]
    }
   ],
   "source": [
    "input_str = common_voice[\"train\"][0][\"text\"]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out special: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f12e2e57-156f-417b-8cfb-69221cc198e8",
   "metadata": {
    "id": "f12e2e57-156f-417b-8cfb-69221cc198e8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00382a3e-abec-4cdd-a54c-d1aaa3ea4707",
   "metadata": {
    "id": "00382a3e-abec-4cdd-a54c-d1aaa3ea4707"
   },
   "source": [
    "Re-loading the first audio sample in the Common Voice dataset will resample\n",
    "it to the desired sampling rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87122d71-289a-466a-afcf-fa354b18946b",
   "metadata": {
    "id": "87122d71-289a-466a-afcf-fa354b18946b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '‡¥∏‡µº‡¥µ‡µç‡¥µ‡¥ï‡¥≤‡¥æ‡¥∂‡¥æ‡¥≤ ‡¥µ‡µà‡¥∏‡µç ‡¥ö‡¥æ‡µª‡¥∏‡¥≤‡µº ‡¥°‡µã. ‡¥ö‡¥®‡µç‡¥¶‡µç‡¥∞‡¥¨‡¥æ‡¥¨‡µÅ‡¥µ‡¥ø‡¥®‡µÅ‡¥Ç ‡¥∏‡¥Ç‡¥≠‡¥µ‡¥Ç ‡¥§‡¥≤‡¥µ‡µá‡¥¶‡¥®‡¥Ø‡¥æ‡¥µ‡µÅ‡¥ï‡¥Ø‡¥æ‡¥£‡µç', 'speaker': 'Sonia', 'audio': {'path': None, 'array': array([ 0.00921631,  0.00930786,  0.00939941, ..., -0.00497437,\n",
      "       -0.00497437, -0.00497437]), 'sampling_rate': 16000}}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91edc72d-08f8-4f01-899d-74e65ce441fc",
   "metadata": {
    "id": "91edc72d-08f8-4f01-899d-74e65ce441fc"
   },
   "source": [
    "Now we can write a function to prepare our data ready for the model:\n",
    "1. We load and resample the audio data by calling `batch[\"audio\"]`. As explained above, ü§ó Datasets performs any necessary resampling operations on the fly.\n",
    "2. We use the feature extractor to compute the log-Mel spectrogram input features from our 1-dimensional audio array.\n",
    "3. We encode the transcriptions to label ids through the use of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6525c478-8962-4394-a1c4-103c54cce170",
   "metadata": {
    "id": "6525c478-8962-4394-a1c4-103c54cce170"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b319fb-2439-4ef6-a70d-a47bf41c4a13",
   "metadata": {
    "id": "70b319fb-2439-4ef6-a70d-a47bf41c4a13"
   },
   "source": [
    "We can apply the data preparation function to all of our training examples using dataset's `.map` method. The argument `num_proc` specifies how many CPU cores to use. Setting `num_proc` > 1 will enable multiprocessing. If the `.map` method hangs with multiprocessing, set `num_proc=1` and process the dataset sequentially.\n",
    "\n",
    "Make yourself some tea üçµ, depending on dataset size, this might take 20-30 minutes ‚è∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b",
   "metadata": {
    "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c7b1cefb884e47a5b84c36f022b122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/27578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be572c",
   "metadata": {
    "id": "c4be572c"
   },
   "outputs": [],
   "source": [
    "common_voice[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a5a58-0239-4a25-b0df-c625fc9c5810",
   "metadata": {
    "id": "263a5a58-0239-4a25-b0df-c625fc9c5810"
   },
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "Now that we've prepared our data, we're ready to dive into the training pipeline.\n",
    "The [ü§ó Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer)\n",
    "will do much of the heavy lifting for us. All we have to do is:\n",
    "\n",
    "- Define a data collator: the data collator takes our pre-processed data and prepares PyTorch tensors ready for the model.\n",
    "\n",
    "- Evaluation metrics: during evaluation, we want to evaluate the model using the [word error rate (WER)](https://huggingface.co/metrics/wer) metric.\n",
    "\n",
    "- Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.\n",
    "\n",
    "- Define the training configuration: this will be used by the ü§ó Trainer to define the training schedule.\n",
    "\n",
    "Once we've fine-tuned the model, we will evaluate it on the test data to verify that we have correctly trained it\n",
    "to transcribe speech in Hindi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d230e6d-624c-400a-bbf5-fa660881df25",
   "metadata": {
    "id": "8d230e6d-624c-400a-bbf5-fa660881df25"
   },
   "source": [
    "### Define a Data Collator\n",
    "\n",
    "The data collator for a sequence-to-sequence speech model is unique in the sense that it\n",
    "treats the `input_features` and `labels` independently: the  `input_features` must be\n",
    "handled by the feature extractor and the `labels` by the tokenizer.\n",
    "\n",
    "The `input_features` are already padded to 30s and converted to a log-Mel spectrogram\n",
    "of fixed dimension by action of the feature extractor, so all we have to do is convert the `input_features`\n",
    "to batched PyTorch tensors. We do this using the feature extractor's `.pad` method with `return_tensors=pt`.\n",
    "\n",
    "The `labels` on the other hand are un-padded. We first pad the sequences\n",
    "to the maximum length in the batch using the tokenizer's `.pad` method. The padding tokens\n",
    "are then replaced by `-100` so that these tokens are **not** taken into account when\n",
    "computing the loss. We then cut the BOS token from the start of the label sequence as we\n",
    "append it later during training.\n",
    "\n",
    "We can leverage the `WhisperProcessor` we defined earlier to perform both the\n",
    "feature extractor and the tokenizer operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5",
   "metadata": {
    "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae7dbf-8a50-456e-a3a8-7fd005390f86",
   "metadata": {
    "id": "3cae7dbf-8a50-456e-a3a8-7fd005390f86"
   },
   "source": [
    "Let's initialise the data collator we've just defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc834702-c0d3-4a96-b101-7b87be32bf42",
   "metadata": {
    "id": "fc834702-c0d3-4a96-b101-7b87be32bf42"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62bb2ab-750a-45e7-82e9-61d6f4805698",
   "metadata": {
    "id": "d62bb2ab-750a-45e7-82e9-61d6f4805698"
   },
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fee1a7-a44c-461e-b047-c3917221572e",
   "metadata": {
    "id": "66fee1a7-a44c-461e-b047-c3917221572e"
   },
   "source": [
    "We'll use the word error rate (WER) metric, the 'de-facto' metric for assessing\n",
    "ASR systems. For more information, refer to the WER [docs](https://huggingface.co/metrics/wer). We'll load the WER metric from ü§ó Evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b22b4011-f31f-4b57-b684-c52332f92890",
   "metadata": {
    "id": "b22b4011-f31f-4b57-b684-c52332f92890"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6558bab-add7-4ab8-8e5b-227078668360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf2a825-6d9f-4a23-b145-c37c0039075b",
   "metadata": {
    "id": "daf2a825-6d9f-4a23-b145-c37c0039075b"
   },
   "source": [
    "###¬†Load a Pre-Trained Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a97fa-4864-476b-8abc-f28b8166cfa5",
   "metadata": {
    "id": "437a97fa-4864-476b-8abc-f28b8166cfa5"
   },
   "source": [
    "Now let's load the pre-trained Whisper checkpoint. Again, this\n",
    "is trivial through use of ü§ó Transformers!\n",
    "\n",
    "To reduce our models memory footprint, we load the model in 8bit, this means we quantize the model to use 1/4th precision (when comapared to float32) with minimal loss to performance. To read more about how this works, head over [here](https://huggingface.co/blog/hf-bitsandbytes-integration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f",
   "metadata": {
    "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f"
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f589cdd4-906b-40b3-a8ee-27f4793f6a97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21af1e9-0188-4134-ac82-defc7bdcc436",
   "metadata": {
    "id": "c21af1e9-0188-4134-ac82-defc7bdcc436"
   },
   "source": [
    "In the final step, we define all the parameters related to training. For more detail on the training arguments, refer to the Seq2SeqTrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a",
   "metadata": {
    "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./test-mal1\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=2,\n",
    "    generation_max_length=128,\n",
    "    # logging_steps=100,\n",
    "    max_steps=100, # only for testing purposes, remove this from your final run :)\n",
    "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
    "    label_names=[\"labels\"],  # same reason as above\n",
    "    report_to=\"wandb\",  # enable logging to W&B\n",
    "    run_name=\"run_two\",  # name of the W&B run (optional)\n",
    "    push_to_hub=True,\n",
    "    logging_steps=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0be96b5-42a5-46f3-8f88-0f9a3d34be6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./test-mal\",  # change to a repo name of your choice\n",
    "#     per_device_train_batch_size=16,\n",
    "#     gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "#     learning_rate=1e-3,\n",
    "#     warmup_steps=50,\n",
    "#     max_steps=4000,\n",
    "#     gradient_checkpointing=True,\n",
    "#     fp16=True,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     predict_with_generate=True,\n",
    "#     generation_max_length=225,\n",
    "#     save_steps=1000,\n",
    "#     eval_steps=1000,\n",
    "#     logging_steps=25,\n",
    "#     report_to=[\"tensorboard\"],\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"wer\",\n",
    "#     greater_is_better=False,\n",
    "#     push_to_hub=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ebc757f-83aa-4c1d-8467-b95d6af9eca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7211bd6e-11b9-485a-bafd-fb116ef2ccbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95f28334-1cfd-4a28-b5f0-7fc05ed9f216",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 13 09:53:40 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  Off  | 00000000:01:01.0 Off |                    0 |\n",
      "| N/A   31C    P0    37W / 250W |  27397MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8iqXhUiuBQCs",
   "metadata": {
    "id": "8iqXhUiuBQCs"
   },
   "source": [
    "Now that our model is fine-tuned, we can push the model on to Hugging Face Hub, this will later help us directly infer the model from the model repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0576aa2a",
   "metadata": {
    "id": "0576aa2a"
   },
   "outputs": [],
   "source": [
    "# peft_model_id = \"kurianbenoy/whisper-large-v2-hindi-100steps\"\n",
    "# model.push_to_hub(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SlyyOGnPgi_I",
   "metadata": {
    "id": "SlyyOGnPgi_I"
   },
   "source": [
    "# Evaluation and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kzfg2qoXgrhg",
   "metadata": {
    "id": "Kzfg2qoXgrhg"
   },
   "source": [
    "On to the fun part, we've successfully fine-tuned our model. Now let's put it to test and calculate the WER on the `test` set.\n",
    "\n",
    "As with training, we do have a few caveats to pay attention to:\n",
    "1. Since we cannot use `predict_with_generate` function, we will hand roll our own eval loop with `torch.cuda.amp.autocast()` you can check it out below.\n",
    "2. Since the base model is frozen, PEFT model sometimes fails to recognise the language while decoding. To fix that, we force the starting tokens to mention the language we are transcribing. This is done via `forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"Marathi\", task=\"transcribe\")` and passing that too the `model.generate` call.\n",
    "\n",
    "That's it, let's get transcribing! üî•\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273a996c",
   "metadata": {
    "id": "273a996c"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\n",
    "\n",
    "peft_model_id = \"kurianbenoy/whisper-large-v2-hindi-100steps\" # Use the same model ID as before.\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ceaa6",
   "metadata": {
    "id": "401ceaa6"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "eval_dataloader = DataLoader(common_voice[\"test\"], batch_size=8, collate_fn=data_collator)\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "normalized_predictions = []\n",
    "normalized_references = []\n",
    "\n",
    "model.eval()\n",
    "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = (\n",
    "                model.generate(\n",
    "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
    "                    forced_decoder_ids=forced_decoder_ids,\n",
    "                    max_new_tokens=255,\n",
    "                )\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
    "            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            predictions.extend(decoded_preds)\n",
    "            references.extend(decoded_labels)\n",
    "            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
    "            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n",
    "        del generated_tokens, labels, batch\n",
    "    gc.collect()\n",
    "wer = 100 * metric.compute(predictions=predictions, references=references)\n",
    "normalized_wer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)\n",
    "eval_metrics = {\"eval/wer\": wer, \"eval/normalized_wer\": normalized_wer}\n",
    "\n",
    "print(f\"{wer=} and {normalized_wer=}\")\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j3XF0PzsCV0v",
   "metadata": {
    "id": "j3XF0PzsCV0v"
   },
   "source": [
    "## Fin!\n",
    "\n",
    "If you made it all the way till the end then pat yourself on the back. Looking back, we learned how to train *any* Whisper checkpoint faster, cheaper and with negligible loss in WER.\n",
    "\n",
    "With PEFT, you can also go beyond Speech recognition and apply the same set of techniques to other pretrained models as well. Come check it out here: https://github.com/huggingface/peft ü§ó\n",
    "\n",
    "Don't forget to tweet your results and tag us! [@huggingface](https://twitter.com/huggingface) and [@reach_vb](https://twitter.com/reach_vb) ‚ù§Ô∏è"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
